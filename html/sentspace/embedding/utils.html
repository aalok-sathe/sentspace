<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>sentspace.embedding.utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sentspace.embedding.utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import pickle
import warnings
from pathlib import Path
import random

import numpy as np
import pandas as pd
import sentspace.utils
from sentspace.utils import io, text
from sentspace.utils.caching import cache_to_disk, cache_to_mem
from sentspace.utils.utils import Word, merge_lists, wordnet
from tqdm import tqdm


# --------- GloVe
def lowercase(f1g):
    &#34;&#34;&#34;
    Return lowercase version of input (assume input is a list of token lists)
    &#34;&#34;&#34;
    return [[token.lower() for token in sent] for sent in f1g]


def get_sent_version(version, df):
    &#34;&#34;&#34;
    Return a list of sentences as lists of tokens given dataframe &amp; version of token to use
    Options for version: &#39;raw&#39;, &#39;cleaned&#39;, &#39;lemmatized&#39;
    &#34;&#34;&#34;
    ref = {&#39;raw&#39;: &#39;Word&#39;, &#39;cleaned&#39;: &#39;Word cleaned&#39;, &#39;lemmatized&#39;: &#39;Word lemma&#39;}
    version = ref[version]
    f1g = []
    for i in df[&#39;Sentence no.&#39;].unique():
        f1g.append(list(df[df[&#39;Sentence no.&#39;] == i].sort_values(&#39;Word no. within sentence&#39;)[version]))
    return f1g


def get_vocab(token_list):
    &#34;&#34;&#34;
    Return set of unique tokens in input (assume input is a list of tokens)
    &#34;&#34;&#34;
    return set(t for t in token_list)


def download_embeddings(which=&#39;glove.840B.300d.txt&#39;):
    raise NotImplementedError()
    if &#39;glove&#39; in which:
        url = &#39;https://huggingface.co/stanfordnlp/glove/resolve/main/glove.840B.300d.zip&#39;


@cache_to_mem
def load_embeddings(emb_file: str = &#39;glove.840B.300d.txt&#39;,
                    data_dir: Path = None,
                    vocab: tuple = ()):
    &#34;&#34;&#34;
    Read through the embedding file to find embeddings for target words in vocab
    Return dict mapping word to embedding (numpy array)
    &#34;&#34;&#34;
    try:
        data_dir = Path(data_dir)
    except TypeError:
        data_dir = Path(__file__).parent / &#39;..&#39; / &#39;..&#39; / &#39;.feature_database/&#39;
    
    vocab = set(vocab)
    OOV = set(vocab)

    io.log(f&#34;loading embeddings from {emb_file} for vocab of size {len(vocab)}&#34;)
    w2v = {}
    with (data_dir / emb_file).open(&#39;r&#39;) as f:
        total_lines = sum(1 for _ in tqdm(f, desc=f&#39;counting # of lines in {data_dir/emb_file}&#39;))
    with (data_dir / emb_file).open(&#39;r&#39;) as f:
        for line in tqdm(f, total=total_lines, desc=f&#39;searching for embeddings in {emb_file}&#39;):
            token, *emb = line.split(&#39; &#39;)
            if token in vocab:
                # print(f&#39;found {token}!&#39;)
                w2v[token] = np.asarray(emb, dtype=float)
                OOV.remove(token)
    
    io.log(f&#34;---done--- loading embeddings from {emb_file}. OOV count: {len(OOV)}/{len(vocab)}&#34;)
    OOVlist = [*OOV]
    random.shuffle(OOVlist)
    io.log(f&#34;           a selection of up to 100 random OOV tokens: {OOVlist[:100]}&#34;)

    return w2v


def get_word_embeds(token_list, w2v, which=&#39;glove&#39;, dims=300, return_NA_words=False, save=False, save_path=False):
    &#34;&#34;&#34;[summary]

    Args:
        tokenized ([type]): [description]
        w2v ([type]): [description]
        which (str, optional): [description]. Defaults to &#39;glove&#39;.
        dims (int, optional): [description]. Defaults to 300.

    Raises:
        ValueError: [description]

    Returns:
        [type]: [description]
    &#34;&#34;&#34;    
    #
    &#34;&#34;&#34;
    Return dataframe of each word, sentence no., and its glove embedding
    If embedding does not exist for a word, fill cells with np.nan
    Parameters:
        f1g: list of sentences as lists of tokens
        w2v: dict mapping word to embedding
        return_NA_words: optionally return unique words that are NA
        save: whether to save results
        save_path: path to save, support .csv &amp; .mat files
    &#34;&#34;&#34;

    embeddings = []
    
    OOV_words = set()
    for token in token_list:
        if token in w2v[which]:
            embeddings.append(w2v[which][token])
        else:
            embeddings.append([np.nan]*dims)
            OOV_words.add(token)
    
    return embeddings



def pool_sentence_embeds(tokens, token_embeddings, filters=[lambda i, x: True],
                         which=&#39;glove&#39;):
    &#34;&#34;&#34;pools embeddings of an entire sentence (given as a list of embeddings)
       using averaging, maxpooling, minpooling, etc., after applying all the
       provided filters as functions (such as content words only).

    Args:
        token_embeddings (list[np.array]): [description]
        filters (list[function[(idx, token) -&gt; bool]], optional): [description]. Defaults to [lambda x: True].
            filters should be functions that map token to bool (e.g. is_content_word(...))
            only tokens that satisfy all filters are retained.

    Returns:
        dict: averaging method -&gt; averaged embedding
    &#34;&#34;&#34;                         

    &#34;&#34;&#34;
    Return dataframe of each sentence no. and its sentence embedding
    from averaging embeddings of words in a sentence (ignore NAs)
    Parameters:
        df: dataframe, output of get_glove_word()
        content_only: if True, use content words only
        is_content_lst: list, values 1 if token is content word, 0 otherwise
        save: whether to save results
        save_path: path to save, support .csv &amp; .mat files
    &#34;&#34;&#34;
    # if content_only:
    #     df = df[np.array(is_content_lst) == 1]

    all_pooled = {}
    for which in token_embeddings:
        filtered_embeds = [e for i, (t, e) in enumerate(zip(tokens, token_embeddings[which]))
                           if all(fn(i, t) for fn in filters)]
        filtered_embeds = np.array(filtered_embeds, dtype=np.float32)
        shape = filtered_embeds.shape

        try:
            filtered_embeds = filtered_embeds[~np.isnan(filtered_embeds[:, 0]), :]
        except IndexError as e:
            pass

        if len(filtered_embeds) == 0:
            io.log(f&#39;all embeddings for current sentence are NaN ({shape} -&gt; {filtered_embeds.shape}): {tokens}&#39;, type=&#39;WARN&#39;)
            filtered_embeds = np.zeros((1,shape[-1]))

        pooled = {
            &#39;pooled_&#39;+which+&#39;_median&#39;: np.median(filtered_embeds, axis=0).reshape(-1).tolist(),
            &#39;pooled_&#39;+which+&#39;_mean&#39;: filtered_embeds.mean(axis=0).reshape(-1).tolist(),
            # &#39;pooled_&#39;+which+&#39;_max&#39;: filtered_embeds.max(axis=0).reshape(-1).tolist(),
            # &#39;pooled_&#39;+which+&#39;_min&#39;: filtered_embeds.min(axis=0).reshape(-1).tolist(),
        }

        all_pooled.update(pooled)

    return all_pooled</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sentspace.embedding.utils.download_embeddings"><code class="name flex">
<span>def <span class="ident">download_embeddings</span></span>(<span>which='glove.840B.300d.txt')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def download_embeddings(which=&#39;glove.840B.300d.txt&#39;):
    raise NotImplementedError()
    if &#39;glove&#39; in which:
        url = &#39;https://huggingface.co/stanfordnlp/glove/resolve/main/glove.840B.300d.zip&#39;</code></pre>
</details>
</dd>
<dt id="sentspace.embedding.utils.get_sent_version"><code class="name flex">
<span>def <span class="ident">get_sent_version</span></span>(<span>version, df)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a list of sentences as lists of tokens given dataframe &amp; version of token to use
Options for version: 'raw', 'cleaned', 'lemmatized'</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_sent_version(version, df):
    &#34;&#34;&#34;
    Return a list of sentences as lists of tokens given dataframe &amp; version of token to use
    Options for version: &#39;raw&#39;, &#39;cleaned&#39;, &#39;lemmatized&#39;
    &#34;&#34;&#34;
    ref = {&#39;raw&#39;: &#39;Word&#39;, &#39;cleaned&#39;: &#39;Word cleaned&#39;, &#39;lemmatized&#39;: &#39;Word lemma&#39;}
    version = ref[version]
    f1g = []
    for i in df[&#39;Sentence no.&#39;].unique():
        f1g.append(list(df[df[&#39;Sentence no.&#39;] == i].sort_values(&#39;Word no. within sentence&#39;)[version]))
    return f1g</code></pre>
</details>
</dd>
<dt id="sentspace.embedding.utils.get_vocab"><code class="name flex">
<span>def <span class="ident">get_vocab</span></span>(<span>token_list)</span>
</code></dt>
<dd>
<div class="desc"><p>Return set of unique tokens in input (assume input is a list of tokens)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_vocab(token_list):
    &#34;&#34;&#34;
    Return set of unique tokens in input (assume input is a list of tokens)
    &#34;&#34;&#34;
    return set(t for t in token_list)</code></pre>
</details>
</dd>
<dt id="sentspace.embedding.utils.get_word_embeds"><code class="name flex">
<span>def <span class="ident">get_word_embeds</span></span>(<span>token_list, w2v, which='glove', dims=300, return_NA_words=False, save=False, save_path=False)</span>
</code></dt>
<dd>
<div class="desc"><p>[summary]</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tokenized</code></strong> :&ensp;<code>[type]</code></dt>
<dd>[description]</dd>
<dt><strong><code>w2v</code></strong> :&ensp;<code>[type]</code></dt>
<dd>[description]</dd>
<dt><strong><code>which</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>[description]. Defaults to 'glove'.</dd>
<dt><strong><code>dims</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>[description]. Defaults to 300.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>[description]</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>[type]</code></dt>
<dd>[description]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_word_embeds(token_list, w2v, which=&#39;glove&#39;, dims=300, return_NA_words=False, save=False, save_path=False):
    &#34;&#34;&#34;[summary]

    Args:
        tokenized ([type]): [description]
        w2v ([type]): [description]
        which (str, optional): [description]. Defaults to &#39;glove&#39;.
        dims (int, optional): [description]. Defaults to 300.

    Raises:
        ValueError: [description]

    Returns:
        [type]: [description]
    &#34;&#34;&#34;    
    #
    &#34;&#34;&#34;
    Return dataframe of each word, sentence no., and its glove embedding
    If embedding does not exist for a word, fill cells with np.nan
    Parameters:
        f1g: list of sentences as lists of tokens
        w2v: dict mapping word to embedding
        return_NA_words: optionally return unique words that are NA
        save: whether to save results
        save_path: path to save, support .csv &amp; .mat files
    &#34;&#34;&#34;

    embeddings = []
    
    OOV_words = set()
    for token in token_list:
        if token in w2v[which]:
            embeddings.append(w2v[which][token])
        else:
            embeddings.append([np.nan]*dims)
            OOV_words.add(token)
    
    return embeddings</code></pre>
</details>
</dd>
<dt id="sentspace.embedding.utils.lowercase"><code class="name flex">
<span>def <span class="ident">lowercase</span></span>(<span>f1g)</span>
</code></dt>
<dd>
<div class="desc"><p>Return lowercase version of input (assume input is a list of token lists)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lowercase(f1g):
    &#34;&#34;&#34;
    Return lowercase version of input (assume input is a list of token lists)
    &#34;&#34;&#34;
    return [[token.lower() for token in sent] for sent in f1g]</code></pre>
</details>
</dd>
<dt id="sentspace.embedding.utils.pool_sentence_embeds"><code class="name flex">
<span>def <span class="ident">pool_sentence_embeds</span></span>(<span>tokens, token_embeddings, filters=[&lt;function &lt;lambda&gt;&gt;], which='glove')</span>
</code></dt>
<dd>
<div class="desc"><p>pools embeddings of an entire sentence (given as a list of embeddings)
using averaging, maxpooling, minpooling, etc., after applying all the
provided filters as functions (such as content words only).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>token_embeddings</code></strong> :&ensp;<code>list[np.array]</code></dt>
<dd>[description]</dd>
</dl>
<p>filters (list[function[(idx, token) -&gt; bool]], optional): [description]. Defaults to [lambda x: True].
filters should be functions that map token to bool (e.g. is_content_word(&hellip;))
only tokens that satisfy all filters are retained.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>averaging method -&gt; averaged embedding</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pool_sentence_embeds(tokens, token_embeddings, filters=[lambda i, x: True],
                         which=&#39;glove&#39;):
    &#34;&#34;&#34;pools embeddings of an entire sentence (given as a list of embeddings)
       using averaging, maxpooling, minpooling, etc., after applying all the
       provided filters as functions (such as content words only).

    Args:
        token_embeddings (list[np.array]): [description]
        filters (list[function[(idx, token) -&gt; bool]], optional): [description]. Defaults to [lambda x: True].
            filters should be functions that map token to bool (e.g. is_content_word(...))
            only tokens that satisfy all filters are retained.

    Returns:
        dict: averaging method -&gt; averaged embedding
    &#34;&#34;&#34;                         

    &#34;&#34;&#34;
    Return dataframe of each sentence no. and its sentence embedding
    from averaging embeddings of words in a sentence (ignore NAs)
    Parameters:
        df: dataframe, output of get_glove_word()
        content_only: if True, use content words only
        is_content_lst: list, values 1 if token is content word, 0 otherwise
        save: whether to save results
        save_path: path to save, support .csv &amp; .mat files
    &#34;&#34;&#34;
    # if content_only:
    #     df = df[np.array(is_content_lst) == 1]

    all_pooled = {}
    for which in token_embeddings:
        filtered_embeds = [e for i, (t, e) in enumerate(zip(tokens, token_embeddings[which]))
                           if all(fn(i, t) for fn in filters)]
        filtered_embeds = np.array(filtered_embeds, dtype=np.float32)
        shape = filtered_embeds.shape

        try:
            filtered_embeds = filtered_embeds[~np.isnan(filtered_embeds[:, 0]), :]
        except IndexError as e:
            pass

        if len(filtered_embeds) == 0:
            io.log(f&#39;all embeddings for current sentence are NaN ({shape} -&gt; {filtered_embeds.shape}): {tokens}&#39;, type=&#39;WARN&#39;)
            filtered_embeds = np.zeros((1,shape[-1]))

        pooled = {
            &#39;pooled_&#39;+which+&#39;_median&#39;: np.median(filtered_embeds, axis=0).reshape(-1).tolist(),
            &#39;pooled_&#39;+which+&#39;_mean&#39;: filtered_embeds.mean(axis=0).reshape(-1).tolist(),
            # &#39;pooled_&#39;+which+&#39;_max&#39;: filtered_embeds.max(axis=0).reshape(-1).tolist(),
            # &#39;pooled_&#39;+which+&#39;_min&#39;: filtered_embeds.min(axis=0).reshape(-1).tolist(),
        }

        all_pooled.update(pooled)

    return all_pooled</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sentspace.embedding" href="index.html">sentspace.embedding</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sentspace.embedding.utils.download_embeddings" href="#sentspace.embedding.utils.download_embeddings">download_embeddings</a></code></li>
<li><code><a title="sentspace.embedding.utils.get_sent_version" href="#sentspace.embedding.utils.get_sent_version">get_sent_version</a></code></li>
<li><code><a title="sentspace.embedding.utils.get_vocab" href="#sentspace.embedding.utils.get_vocab">get_vocab</a></code></li>
<li><code><a title="sentspace.embedding.utils.get_word_embeds" href="#sentspace.embedding.utils.get_word_embeds">get_word_embeds</a></code></li>
<li><code><a title="sentspace.embedding.utils.lowercase" href="#sentspace.embedding.utils.lowercase">lowercase</a></code></li>
<li><code><a title="sentspace.embedding.utils.pool_sentence_embeds" href="#sentspace.embedding.utils.pool_sentence_embeds">pool_sentence_embeds</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>