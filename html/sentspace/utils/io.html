<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>sentspace.utils.io API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>sentspace.utils.io</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
import pickle
import textwrap
from datetime import date
from hashlib import sha1
from pathlib import Path
from sys import stderr, stdout
import pandas as pd
import numpy as np
import sentspace.utils
from sentspace.utils.caching import cache_to_disk, cache_to_mem
from sentspace.utils.s3 import load_feature
from tqdm import tqdm

# from sentspace.utils.sanity_checks import sanity_check_databases

def dump_features(): pass

def create_output_paths(input_file:str, output_dir:str, calling_module=None, stop_words_file:str=None) -&gt; Path:
    embed_method = &#39;all&#39;  # options: &#39;strict&#39;, &#39;all&#39;
    content_only = False

    output_dir = Path(output_dir)

    suffix = &#39;&#39;

    # output will be organized based on its respective input file,
    # together with a hash of the contents (for completeness)
    out_file_name = os.path.basename(input_file).split(&#39;.&#39;)[0]
    # with open(input_file, &#39;r&#39;) as f:
    output_dir /= (out_file_name + &#39;_md5:&#39; + sentspace.utils.md5(input_file))
    output_dir /= calling_module or &#39;&#39;
    output_dir /= date.today().strftime(&#39;run_%Y-%m-%d&#39;)
    output_dir.mkdir(parents=True, exist_ok=True)

    return output_dir

    sent_suffix = f&#34;_{embed_method}&#34;

    if content_only:
        sent_suffix = &#39;_content&#39; + sent_suffix
    if stop_words_file:
        sent_suffix = &#39;_content&#39;+&#39;_minus_stop_words&#39; + sent_suffix
    # Make out outlex path
    word_lex_output_path, embed_lex_output_path, plot_path, na_words_path, bench_perc_out_path = _create_output_paths(
        output_dir, out_file_name, &#39;lexical&#39;, sent_suffix=sent_suffix)
    # Make output syntax path
    _, sent_output_path = _create_output_paths(
        output_dir, out_file_name, &#39;syntax&#39;, sent_suffix=sent_suffix)
    glove_words_output_path, glove_sents_output_path = _create_output_paths(
        output_dir, out_file_name, &#39;embedding&#39;, sent_suffix=sent_suffix)
    pmi_paths = _create_output_paths(
        output_dir, out_file_name, &#39;PMI&#39;, sent_suffix=sent_suffix)

    # lex_base = f&#39;analysis_example/{date_}\\lex\\&#39;

    return sent_output_path, glove_words_output_path, glove_sents_output_path


# def _create_output_paths(output_dir:Path, name, analysis, suffix=&#39;&#39;, sent_suffix=&#39;&#39;):
#     &#34;&#34;&#34;
#     Return list of file paths and create output folder if appropriate
#     Supports analysis = &#39;lex&#39;, &#39;glove&#39;,&#39;syntax&#39;,&#39;PMI&#39;
#     &#34;&#34;&#34;
#     if analysis == &#39;lexical&#39;:
#         return_paths = [f&#34;{name}_lex_features_words{suffix}.csv&#34;,
#                         f&#34;{name}_lex_features_sents{suffix}{sent_suffix}.csv&#34;,
#                         f&#34;{name}_plots{suffix}{sent_suffix}_&#34;,
#                         f&#34;{name}_unique_NA{suffix}{sent_suffix}.csv&#34;,
#                         f&#34;{name}_benchmark_percentiles{suffix}{sent_suffix}.csv&#34;]
#     elif analysis == &#39;embedding&#39;:
#         return_paths = [f&#34;{name}_glove_words{suffix}.csv&#34;,
#                         f&#34;{name}_glove_sents{suffix}{sent_suffix}.csv&#34;]
#     elif analysis == &#39;PMI&#39;:
#         return_paths = [f&#34;{name}_pPMI_0{suffix}.csv&#34;,
#                         f&#34;{name}_pPMI_1{suffix}.csv&#34;,
#                         f&#34;{name}_pPMI_2{suffix}.csv&#34;,
#                         f&#34;{name}_ngrams{suffix}.pkl&#34;,
#                         f&#34;{name}_nm1grams{suffix}.pkl&#34;]
#     elif analysis == &#39;syntax&#39;:
#         return_paths = [f&#34;{name}_{suffix}.csv&#34;,
#                         f&#34;{name}_{suffix}{sent_suffix}.csv&#34;]
#     elif analysis == &#39;lex_per_word&#39;:
#         return_paths = [f&#34;{name}_{suffix}.csv&#34;,
#                         f&#34;{name}_{suffix}{sent_suffix}.csv&#34;]
#     else:
#         raise ValueError(&#39;Invalid analysis method!&#39;)
#     output_dir = os.path.join(output_dir, analysis)
#     if not os.path.isdir(output_dir):  # create output_folder if it doesn&#39;t exist
#         Path(output_dir).mkdir(parents=True, exist_ok=True)

#     result = [os.path.join(output_dir, path) for path in return_paths]
#     return result


def read_sentences(filename: str, stop_words_file: str = None):
    &#34;&#34;&#34;reads sentences from a file, one per line, filtering
        for stopwords

    Args:
        filename (str): path to input file containing sentences
        stop_words_file (str, optional): path to file containing stopwords

    Returns:
        list[list]: list of the tokens from each sentences, nested as a list
        list: list of sentences
    &#34;&#34;&#34;
    

    # if a non-empty collection of stop words has been supplied
    if stop_words_file:
        raise NotImplementedError()
        stop_words = set(np.loadtxt(stop_words_file, delimiter=&#39;\t&#39;, unpack=False, dtype=str))

    if filename.endswith(&#39;.txt&#39;):
        UIDs = []
        token_lists = []
        sentences = []
        with open(filename, &#39;r&#39;) as f:
            UID_prefix = f&#39;{filename[-8:]:#&gt;10}&#39; + &#39;_&#39; + sentspace.utils.md5(filename)[-5:]
            for i, line in enumerate(f):

                if line.strip():
                    tokens = sentspace.utils.text.tokenize(line) # line.split()
                else:
                    continue

                UIDs += [UID_prefix + &#39;_&#39; + f&#39;{len(UIDs):0&gt;5}&#39;]
                if stop_words_file:
                    filtered = [x for x in tokens if x not in stop_words]
                    token_lists.append(filtered)
                    sentences.append(&#39; &#39;.join(filtered))
                # no stopwords supplied; do not filter
                else:
                    token_lists.append(tokens)
                    sentences.append(line.strip())
        return UIDs, token_lists, sentences

    elif filename.endswith(&#39;.pkl&#39;):
        df = pd.read_pickle(filename)
    elif filename.endswith(&#39;.csv&#39;):
        df = pd.read_csv(filename, sep=&#39;,&#39;)
    elif filename.endswith(&#39;.tsv&#39;):
        df = pd.read_csv(filename, sep=&#39;\t&#39;)
    else:
        raise ValueError(&#39;unknown type of file supplied (must be txt/pkl/csv/tsv. if pickle, must be a dataframe object)&#39;)

    try:
        UIDs = df[&#39;index&#39;].tolist()
    except KeyError:
        UIDs = df.index.tolist()
    except AttributeError:
        raise(&#39;does your dataframe have a unique index for each sentence?&#39;)

    sentences = df[&#39;sentence&#39;].tolist()
    token_lists = df[&#39;sentence&#39;].apply(lambda s: sentspace.utils.text.tokenize(s)).tolist()

    return UIDs, token_lists, sentences



def load_surprisal(file=&#39;pickle/surprisal-3_dict.pkl&#39;):
    &#34;&#34;&#34;
    Load dict mapping word to surprisal value
    &#34;&#34;&#34;
    with open(file, &#39;rb&#39;) as f:
        return pickle.load(f)


def log(message, type=&#39;INFO&#39;):
    timestamp = f&#39;{sentspace.utils.time() - sentspace.utils.START_TIME():.2f}s&#39;
    lines = textwrap.wrap(message, width=120, initial_indent=&#39;=&#39;*4 + f&#39; [{type} @ {timestamp}] &#39;, subsequent_indent=&#39;=&#39;*8+&#39; &#39;)
    tqdm.write(&#39;\n&#39;.join(lines), file=stderr)
    # print(*lines, sep=&#39;\n&#39;, file=stderr)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sentspace.utils.io.create_output_paths"><code class="name flex">
<span>def <span class="ident">create_output_paths</span></span>(<span>input_file: str, output_dir: str, calling_module=None, stop_words_file: str = None) ‑> pathlib.Path</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_output_paths(input_file:str, output_dir:str, calling_module=None, stop_words_file:str=None) -&gt; Path:
    embed_method = &#39;all&#39;  # options: &#39;strict&#39;, &#39;all&#39;
    content_only = False

    output_dir = Path(output_dir)

    suffix = &#39;&#39;

    # output will be organized based on its respective input file,
    # together with a hash of the contents (for completeness)
    out_file_name = os.path.basename(input_file).split(&#39;.&#39;)[0]
    # with open(input_file, &#39;r&#39;) as f:
    output_dir /= (out_file_name + &#39;_md5:&#39; + sentspace.utils.md5(input_file))
    output_dir /= calling_module or &#39;&#39;
    output_dir /= date.today().strftime(&#39;run_%Y-%m-%d&#39;)
    output_dir.mkdir(parents=True, exist_ok=True)

    return output_dir

    sent_suffix = f&#34;_{embed_method}&#34;

    if content_only:
        sent_suffix = &#39;_content&#39; + sent_suffix
    if stop_words_file:
        sent_suffix = &#39;_content&#39;+&#39;_minus_stop_words&#39; + sent_suffix
    # Make out outlex path
    word_lex_output_path, embed_lex_output_path, plot_path, na_words_path, bench_perc_out_path = _create_output_paths(
        output_dir, out_file_name, &#39;lexical&#39;, sent_suffix=sent_suffix)
    # Make output syntax path
    _, sent_output_path = _create_output_paths(
        output_dir, out_file_name, &#39;syntax&#39;, sent_suffix=sent_suffix)
    glove_words_output_path, glove_sents_output_path = _create_output_paths(
        output_dir, out_file_name, &#39;embedding&#39;, sent_suffix=sent_suffix)
    pmi_paths = _create_output_paths(
        output_dir, out_file_name, &#39;PMI&#39;, sent_suffix=sent_suffix)

    # lex_base = f&#39;analysis_example/{date_}\\lex\\&#39;

    return sent_output_path, glove_words_output_path, glove_sents_output_path</code></pre>
</details>
</dd>
<dt id="sentspace.utils.io.dump_features"><code class="name flex">
<span>def <span class="ident">dump_features</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump_features(): pass</code></pre>
</details>
</dd>
<dt id="sentspace.utils.io.load_surprisal"><code class="name flex">
<span>def <span class="ident">load_surprisal</span></span>(<span>file='pickle/surprisal-3_dict.pkl')</span>
</code></dt>
<dd>
<div class="desc"><p>Load dict mapping word to surprisal value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_surprisal(file=&#39;pickle/surprisal-3_dict.pkl&#39;):
    &#34;&#34;&#34;
    Load dict mapping word to surprisal value
    &#34;&#34;&#34;
    with open(file, &#39;rb&#39;) as f:
        return pickle.load(f)</code></pre>
</details>
</dd>
<dt id="sentspace.utils.io.log"><code class="name flex">
<span>def <span class="ident">log</span></span>(<span>message, type='INFO')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log(message, type=&#39;INFO&#39;):
    timestamp = f&#39;{sentspace.utils.time() - sentspace.utils.START_TIME():.2f}s&#39;
    lines = textwrap.wrap(message, width=120, initial_indent=&#39;=&#39;*4 + f&#39; [{type} @ {timestamp}] &#39;, subsequent_indent=&#39;=&#39;*8+&#39; &#39;)
    tqdm.write(&#39;\n&#39;.join(lines), file=stderr)
    # print(*lines, sep=&#39;\n&#39;, file=stderr)</code></pre>
</details>
</dd>
<dt id="sentspace.utils.io.read_sentences"><code class="name flex">
<span>def <span class="ident">read_sentences</span></span>(<span>filename: str, stop_words_file: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>reads sentences from a file, one per line, filtering
for stopwords</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>path to input file containing sentences</dd>
<dt><strong><code>stop_words_file</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>path to file containing stopwords</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list[list]</code></dt>
<dd>list of the tokens from each sentences, nested as a list</dd>
<dt><code>list</code></dt>
<dd>list of sentences</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_sentences(filename: str, stop_words_file: str = None):
    &#34;&#34;&#34;reads sentences from a file, one per line, filtering
        for stopwords

    Args:
        filename (str): path to input file containing sentences
        stop_words_file (str, optional): path to file containing stopwords

    Returns:
        list[list]: list of the tokens from each sentences, nested as a list
        list: list of sentences
    &#34;&#34;&#34;
    

    # if a non-empty collection of stop words has been supplied
    if stop_words_file:
        raise NotImplementedError()
        stop_words = set(np.loadtxt(stop_words_file, delimiter=&#39;\t&#39;, unpack=False, dtype=str))

    if filename.endswith(&#39;.txt&#39;):
        UIDs = []
        token_lists = []
        sentences = []
        with open(filename, &#39;r&#39;) as f:
            UID_prefix = f&#39;{filename[-8:]:#&gt;10}&#39; + &#39;_&#39; + sentspace.utils.md5(filename)[-5:]
            for i, line in enumerate(f):

                if line.strip():
                    tokens = sentspace.utils.text.tokenize(line) # line.split()
                else:
                    continue

                UIDs += [UID_prefix + &#39;_&#39; + f&#39;{len(UIDs):0&gt;5}&#39;]
                if stop_words_file:
                    filtered = [x for x in tokens if x not in stop_words]
                    token_lists.append(filtered)
                    sentences.append(&#39; &#39;.join(filtered))
                # no stopwords supplied; do not filter
                else:
                    token_lists.append(tokens)
                    sentences.append(line.strip())
        return UIDs, token_lists, sentences

    elif filename.endswith(&#39;.pkl&#39;):
        df = pd.read_pickle(filename)
    elif filename.endswith(&#39;.csv&#39;):
        df = pd.read_csv(filename, sep=&#39;,&#39;)
    elif filename.endswith(&#39;.tsv&#39;):
        df = pd.read_csv(filename, sep=&#39;\t&#39;)
    else:
        raise ValueError(&#39;unknown type of file supplied (must be txt/pkl/csv/tsv. if pickle, must be a dataframe object)&#39;)

    try:
        UIDs = df[&#39;index&#39;].tolist()
    except KeyError:
        UIDs = df.index.tolist()
    except AttributeError:
        raise(&#39;does your dataframe have a unique index for each sentence?&#39;)

    sentences = df[&#39;sentence&#39;].tolist()
    token_lists = df[&#39;sentence&#39;].apply(lambda s: sentspace.utils.text.tokenize(s)).tolist()

    return UIDs, token_lists, sentences</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="sentspace.utils" href="index.html">sentspace.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sentspace.utils.io.create_output_paths" href="#sentspace.utils.io.create_output_paths">create_output_paths</a></code></li>
<li><code><a title="sentspace.utils.io.dump_features" href="#sentspace.utils.io.dump_features">dump_features</a></code></li>
<li><code><a title="sentspace.utils.io.load_surprisal" href="#sentspace.utils.io.load_surprisal">load_surprisal</a></code></li>
<li><code><a title="sentspace.utils.io.log" href="#sentspace.utils.io.log">log</a></code></li>
<li><code><a title="sentspace.utils.io.read_sentences" href="#sentspace.utils.io.read_sentences">read_sentences</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>