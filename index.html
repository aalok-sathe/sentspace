<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>sentspace API documentation</title>
<meta name="description" content="Sentspace 0.0.1 (C) 2020-2021 EvLab &lt;evlab.mit.edu&gt;, MIT BCS. All rights reserved …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>sentspace</code></h1>
</header>
<section id="section-intro">
<h3 id="sentspace-001-c-2020-2021-evlab-mit-bcs-all-rights-reserved">Sentspace 0.0.1 (C) 2020-2021 EvLab <evlab.mit.edu>, MIT BCS. All rights reserved.</h3>
<p>Homepage: <a href="https://github.com/aalok-sathe/sentspace">https://github.com/aalok-sathe/sentspace</a></p>
<p>Authors (reverse alphabet.; insert <code>@</code> symbol for valid email):</p>
<ul>
<li>Greta Tuckute <code>&lt;gretatu % mit.edu&gt;</code></li>
<li>Josef Affourtit <code>&lt;jaffourt % mit.edu&gt;</code></li>
<li>Alvince Pongos <code>&lt;apongos % mit.edu&gt;</code></li>
<li>Aalok Sathe <code>&lt;asathe % mit.edu&gt;</code></li>
</ul>
<p>Please contact any of the following with questions about the code or license.</p>
<ul>
<li>Aalok Sathe <code>&lt;asathe % mit.edu&gt;</code> </li>
<li>Greta Tuckute <code>&lt;gretatu % mit.edu&gt;</code></li>
</ul>
<h1 id="sentspace">sentspace</h1>
<!-- ABOUT THE PROJECT -->
<h2 id="about-the-project">About The Project</h2>
<p><code><a title="sentspace" href="#sentspace">sentspace</a></code>
aims to characterize language stimuli using a collection of metrics and comparisons.
Imagine you generated a set of sentences based on some optimization algorithm or extracted
them from an ANN language model. How do these 'artificially' generated sentences compare to
naturally occurring sentences?</p>
<p>In the present form of <code><a title="sentspace" href="#sentspace">sentspace</a></code>,
the goal is for a user to feed the toolbox sentences and for the toolbox to return sentence feature values
along the many metrics it implements (and will implementat in the future).</p>
<h2 id="documentation">Documentation</h2>
<p><a href="https://circleci.com/gh/aalok-sathe/sentspace/tree/main"><img alt="CircleCI" src="https://circleci.com/gh/aalok-sathe/sentspace/tree/main.svg?style=svg"></a></p>
<p>For more information, <a href="https://aalok-sathe.github.io/sentspace/index.html">visit the docs!</a></p>
<!-- request read access to the [project doc](https://docs.google.com/document/d/1O1M7T5Ji6KKRvDfI7KQXe_LJ7l9O6_OZA7TEaVP4f8E/edit#). -->
<h2 id="usage">Usage</h2>
<p>The recommended way to run this project with all its dependencies is using a prebuilt Docker image, <code>aloxatel/ubuntu:sent-space</code>.
To use the image as a container using <code>singularity</code>, do:</p>
<h4 id="first-some-important-housekeeping-stuff"><strong>first, some important housekeeping stuff</strong></h4>
<ul>
<li><code>which singularity</code> (make sure you have singularity, or load/install it otherwise)</li>
<li>make sure you have set the ennvironment variables that specify where <code>singularity</code> will cache its images. if you don't do this, <code>singularity</code> will make assumptions and you may end up with a full disk and an unresponsive server. you need about 6gb of free space at the target location.</li>
</ul>
<h4 id="next-running-the-container-automatically-built-and-deployed-to-docker-hub"><strong>next, running the container</strong> (automatically built and deployed to Docker hub)</h4>
<p><a href="https://circleci.com/gh/aalok-sathe/sentspace/tree/circle-ci"><img alt="CircleCI" src="https://circleci.com/gh/aalok-sathe/sentspace/tree/circle-ci.svg?style=svg"></a></p>
<ul>
<li><code>singularity shell docker://aloxatel/ubuntu:sent-space</code> (or alternatively, from the root of the repo, <code>bash singularity-shell.sh</code>). this step can take a while when you run it for the first time as it needs to download the image from docker hub and convert it to singularity image format (<code>.sif</code>). however, each subsequent run will execute rapidly.</li>
<li>[now you are within the container] <code>source .singularitybashrc</code>, again from the root of the repo, to activate the environment variables and so on.</li>
<li>now you are ready to run the module!</li>
</ul>
<p>For a complete list of options and default values, see the <code>help</code> page like so:</p>
<pre><code class="language-bash">python3 -m sentspace -h
</code></pre>
<h3 id="submodules">Submodules</h3>
<p>In general, each submodule exists as a standalone implementation. You can run each module on its own by specifying it like so:
<code>python -m sentspace.syntax -h</code>, which will print out the usage for that submodule.
Below, we provide more information and the capabilities/usage of each submodule in some greater depth.</p>
<h4 id="sentspacesyntax"><code><a title="sentspace.syntax" href="syntax/index.html">sentspace.syntax</a></code></h4>
<h4 id="sentspacelexical"><code><a title="sentspace.lexical" href="lexical/index.html">sentspace.lexical</a></code></h4>
<p>Not yet implemented</p>
<h4 id="sentspaceembedding"><code><a title="sentspace.embedding" href="embedding/index.html">sentspace.embedding</a></code></h4>
<p>Not yet implemented</p>
<h4 id="semantic"><code>semantic</code></h4>
<p>Not yet implemented</p>
<!-- CONTRIBUTING -->
<h2 id="contributing">Contributing</h2>
<p>Any contributions you make are <strong>greatly appreciated</strong>, and no contribution is <em>too small</em> to contribute.</p>
<ol>
<li>Fork the Project</li>
<li>Create your Feature Branch (<code>git checkout -b feature/AmazingFeature</code>)</li>
<li>Commit your Changes (<code>git commit -m 'Add some AmazingFeature'</code>)</li>
<li>Push to the Branch (<code>git push origin feature/AmazingFeature</code>)</li>
<li>Open a Pull Request</li>
</ol>
<!-- LICENSE -->
<h2 id="license">License</h2>
<p>MIT License.</p>
<!-- CONTACT -->
<h2 id="contact">Contact</h2>
<ul>
<li>About the project: </li>
<li>Greta Tuckute, EvLab, MIT BCS</li>
<li>For help:</li>
<li>coming soon</li>
</ul>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
    ### Sentspace 0.0.1 (C) 2020-2021 EvLab &lt;evlab.mit.edu&gt;, MIT BCS. All rights reserved.

    Homepage: https://github.com/aalok-sathe/sentspace

    Authors (reverse alphabet.; insert `@` symbol for valid email):
    
    - Greta Tuckute `&lt;gretatu % mit.edu&gt;`
    - Josef Affourtit `&lt;jaffourt % mit.edu&gt;`
    - Alvince Pongos `&lt;apongos % mit.edu&gt;`
    - Aalok Sathe `&lt;asathe % mit.edu&gt;`

    Please contact any of the following with questions about the code or license.
    
    - Aalok Sathe `&lt;asathe % mit.edu&gt;` 
    - Greta Tuckute `&lt;gretatu % mit.edu&gt;`

    .. include:: ../README.md
&#39;&#39;&#39;

__pdoc__ = {&#39;semantic&#39;: False,
            }


from pathlib import Path


import sentspace.utils as utils
import sentspace.syntax as syntax
import sentspace.lexical as lexical
import sentspace.semantic as semantic
import sentspace.embedding as embedding
# ...



def run_sentence_features_pipeline(input_file: str, stop_words_file: str = None,
                                   benchmark_file: str = None, output_dir: str = None,
                                   output_format: str = None,
                                   lexical: bool = False, syntax: bool = False,
                                   embedding: bool = False, semantic: bool = False,
                                   #
                                   emb_data_dir: str = None) -&gt; Path:
    &#34;&#34;&#34;
    Runs the full sentence features pipeline on the given input according to
    requested submodules (currently supported: `lexical`, `syntax`, `embedding`,
    indicated by boolean flags).
        
    Returns an instance of `Path` pointing to the output directory resulting from this
    run of the full pipeline. The output directory contains Pickled or TSVed pandas 
    DataFrames containing the requested features.


    Args:
        input_file (str): path to input text file containing sentences
                            one per line [required]
        stop_words_file (str): path to text file containing stopwords to filter
                                out, one per line [optional]
        benchmark_file (str): path to a file containing a benchmark corpus to
                                compare the current input against; e.g. UD [optional]
        
        {lexical,syntax,embedding,semantic,...} (bool): compute submodule features? [False]
    &#34;&#34;&#34;

    # lock = multiprocessing.Manager().Lock()

    # create output folder
    utils.io.log(&#39;creating output folder&#39;)
    # (sent_output_path,
    #  glove_words_output_path,
    #  glove_sents_output_path)
    output_dir = utils.io.create_output_paths(input_file,
                                              output_dir=output_dir,
                                              stop_words_file=stop_words_file)
    # with (output_dir / &#39;config.txt&#39;).open(&#39;w+&#39;) as f:
    #     print(args, file=f)

    utils.io.log(&#39;reading input sentences&#39;)
    UIDs, token_lists, sentences = utils.io.read_sentences(input_file, stop_words_file=stop_words_file)
    utils.io.log(&#39;---done--- reading input sentences&#39;)

    # surprisal_database = &#39;pickle/surprisal-3_dict.pkl&#39; # default: 3-gram surprisal
    # features_ignore_case = True
    # features_transform = [&#39;default&#39;, None, None] # format: [method, cols to log transform, cols to z-score] (if method is specified, the other two are ignored)

    # Get morpheme from polyglot library instead of library
    # TODO: where was this supposed to be used?
    # poly_morphemes = utils.get_poly_morpheme(flat_sentence_num, flat_token_list)

    if lexical:
        utils.io.log(&#39;*** running lexical submodule pipeline&#39;)
        _ = lexical.utils.load_databases(features=&#39;all&#39;)

        # lexical_features = [sentspace.lexical.get_features(sentence, identifier=UIDs[i])
        #                     for i, sentence in enumerate(tqdm(sentences, desc=&#39;Lexical pipeline&#39;))]
        lexical_features = utils.parallelize(lexical.get_features, sentences, UIDs,
                                             wrap_tqdm=True, desc=&#39;Lexical pipeline&#39;)

        lexical_out = output_dir / &#39;lexical&#39;
        lexical_out.mkdir(parents=True, exist_ok=True)

        # with (lexical_out/&#39;token-features.json&#39;).open(&#39;w&#39;) as f:
        #       json.dump(lexical_features, f)

        # lexical is a special case since it returns dicts per token (rather than per sentence)
        # so we want to flatten it so that pandas creates a sensible dataframe from it.
        token_df = pd.DataFrame(chain.from_iterable(lexical_features))

        utils.io.log(f&#39;outputting lexical token dataframe to {lexical_out}&#39;)
        if output_format == &#39;tsv&#39;:
            token_df.to_csv(lexical_out / &#39;token-features.tsv&#39;, sep=&#39;\t&#39;, index=False)
        if output_format == &#39;pkl&#39;:
            token_df.to_pickle(lexical_out / &#39;token-features.pkl.gz&#39;, protocol=5)

        utils.io.log(f&#39;--- finished lexical pipeline&#39;)

    if syntax:
        utils.io.log(&#39;*** running syntax submodule pipeline&#39;)
        syntax_features = [syntax.get_features(sentence, dlt=True, left_corner=True, identifier=UIDs[i])
                           for i, sentence in enumerate(tqdm(sentences, desc=&#39;Syntax pipeline&#39;))]
        # syntax_features = utils.parallelize(sentspace.syntax.get_features, sentences, UIDs,
        #                                     dlt=True, left_corner=True,
        #                                     wrap_tqdm=True, desc=&#39;Syntax pipeline&#39;)

        syntax_out = output_dir / &#39;syntax&#39;
        syntax_out.mkdir(parents=True, exist_ok=True)

        # with (syntax_out/&#39;features.json&#39;).open(&#39;w&#39;) as f:
        #       f.write(str(syntax_features))

        # put all features in the sentence df except the token-level ones
        token_syntax_features = {&#39;dlt&#39;, &#39;leftcorner&#39;}
        sentence_df = pd.DataFrame([{k: v for k, v in feature_dict.items() if k not in token_syntax_features}
                                    for feature_dict in syntax_features], index=UIDs)

        # output gives us dataframes corresponding to each token-level feature. we need to combine these
        # into a single dataframe
        # we use functools.reduce to apply the pd.concat function to all the dataframes and join dataframes
        # that contain different features for the same tokens
        # we use df.T.drop_duplicates().T to remove duplicate columns (&#39;token&#39;, &#39;sentence&#39;, &#39;index&#39; etc) that appear in
        # all/multiple dataframes as part of the standard output schema
        token_dfs = [reduce(lambda x, y: pd.concat([x, y], axis=1, sort=False),
                            (v for k, v in feature_dict.items() if k in token_syntax_features)).T.drop_duplicates().T
                     for feature_dict in syntax_features]
        token_df = reduce(lambda x, y: pd.concat([x, y], ignore_index=True), token_dfs)

        utils.io.log(f&#39;outputting syntax dataframes to {syntax_out}&#39;)
        if output_format == &#39;tsv&#39;:
            sentence_df.to_csv(syntax_out / &#39;sentence-features.tsv&#39;, sep=&#39;\t&#39;, index=False)
            token_df.to_csv(syntax_out / &#39;token-features.tsv&#39;, sep=&#39;\t&#39;, index=False)
        elif output_format == &#39;pkl&#39;:
            sentence_df.to_pickle(syntax_out / &#39;sentence-features.pkl.gz&#39;, protocol=5)
            token_df.to_pickle(syntax_out / &#39;token-features.pkl.gz&#39;, protocol=5)

        utils.io.log(f&#39;--- finished syntax pipeline&#39;)

    # Calculate PMI
    # utils.GrabNGrams(sent_rows,pmi_paths)
    # utils.pPMI(sent_rows, pmi_paths)
    # pdb.set_trace()

    if embedding:
        utils.io.log(&#39;*** running embedding submodule pipeline&#39;)
        # Get GloVE

        stripped_words = utils.text.strip_words(chain(*token_lists), method=&#39;punctuation&#39;)
        vocab = embedding.utils.get_vocab(stripped_words)
        _ = embedding.utils.load_embeddings(emb_file=&#39;glove.840B.300d.txt&#39;,
                                                      vocab=(*sorted(vocab),),
                                                      data_dir=emb_data_dir)

        # embedding_features = [sentspace.embedding.get_features(sentence, vocab=vocab, data_dir=emb_data_dir,
        #                                                        identifier=UIDs[i])
        #                        for i, sentence in enumerate(tqdm(sentences, desc=&#39;Embedding pipeline&#39;))]
        embedding_features = utils.parallelize(embedding.get_features, sentences, UIDs,
                                               vocab=vocab, data_dir=emb_data_dir,
                                               wrap_tqdm=True, desc=&#39;Embedding pipeline&#39;)

        embedding_out = output_dir / &#39;embedding&#39;
        embedding_out.mkdir(parents=True, exist_ok=True)

        sentence_df = pd.DataFrame([{k: v for k, v in feature_dict.items() if k != &#39;token_embeds&#39;}
                                    for feature_dict in embedding_features])

        utils.io.log(f&#39;outputting embedding dataframe(s) to {embedding_out}&#39;)
        if output_format == &#39;tsv&#39;:
            sentence_df.to_csv(embedding_out / &#39;sentence-features.tsv&#39;, sep=&#39;\t&#39;, index=False)
        elif output_format == &#39;pkl&#39;:
            sentence_df.to_pickle(embedding_out / &#39;sentence-features.pkl.gz&#39;, protocol=5)

        # token_dfs = [feature_dict[&#39;token_embeds&#39;] for feature_dict in embedding_features]
        # token_df = reduce(lambda x, y: pd.concat([x, y], ignore_index=True), token_dfs)

        # utils.io.log(f&#39;outputting embedding token dataframe to {embedding_out}&#39;)
        # token_df.to_csv(embedding_out / &#39;token-features.tsv&#39;, sep=&#39;\t&#39;, index=False)

        utils.io.log(f&#39;--- finished embedding pipeline&#39;)

    # Plot input data to benchmark data
    #utils.plot_usr_input_against_benchmark_dist_plots(df_benchmark, sent_embed)

    return output_dir</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="sentspace.embedding" href="embedding/index.html">sentspace.embedding</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="sentspace.lexical" href="lexical/index.html">sentspace.lexical</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="sentspace.syntax" href="syntax/index.html">sentspace.syntax</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="sentspace.utils" href="utils/index.html">sentspace.utils</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sentspace.run_sentence_features_pipeline"><code class="name flex">
<span>def <span class="ident">run_sentence_features_pipeline</span></span>(<span>input_file: str, stop_words_file: str = None, benchmark_file: str = None, output_dir: str = None, output_format: str = None, lexical: bool = False, syntax: bool = False, embedding: bool = False, semantic: bool = False, emb_data_dir: str = None) ‑> pathlib.Path</span>
</code></dt>
<dd>
<div class="desc"><p>Runs the full sentence features pipeline on the given input according to
requested submodules (currently supported: <code><a title="sentspace.lexical" href="lexical/index.html">sentspace.lexical</a></code>, <code><a title="sentspace.syntax" href="syntax/index.html">sentspace.syntax</a></code>, <code><a title="sentspace.embedding" href="embedding/index.html">sentspace.embedding</a></code>,
indicated by boolean flags).</p>
<p>Returns an instance of <code>Path</code> pointing to the output directory resulting from this
run of the full pipeline. The output directory contains Pickled or TSVed pandas
DataFrames containing the requested features.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_file</code></strong> :&ensp;<code>str</code></dt>
<dd>path to input text file containing sentences
one per line [required]</dd>
<dt><strong><code>stop_words_file</code></strong> :&ensp;<code>str</code></dt>
<dd>path to text file containing stopwords to filter
out, one per line [optional]</dd>
<dt><strong><code>benchmark_file</code></strong> :&ensp;<code>str</code></dt>
<dd>path to a file containing a benchmark corpus to
compare the current input against; e.g. UD [optional]</dd>
</dl>
<p>{lexical,syntax,embedding,semantic,&hellip;} (bool): compute submodule features? [False]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_sentence_features_pipeline(input_file: str, stop_words_file: str = None,
                                   benchmark_file: str = None, output_dir: str = None,
                                   output_format: str = None,
                                   lexical: bool = False, syntax: bool = False,
                                   embedding: bool = False, semantic: bool = False,
                                   #
                                   emb_data_dir: str = None) -&gt; Path:
    &#34;&#34;&#34;
    Runs the full sentence features pipeline on the given input according to
    requested submodules (currently supported: `lexical`, `syntax`, `embedding`,
    indicated by boolean flags).
        
    Returns an instance of `Path` pointing to the output directory resulting from this
    run of the full pipeline. The output directory contains Pickled or TSVed pandas 
    DataFrames containing the requested features.


    Args:
        input_file (str): path to input text file containing sentences
                            one per line [required]
        stop_words_file (str): path to text file containing stopwords to filter
                                out, one per line [optional]
        benchmark_file (str): path to a file containing a benchmark corpus to
                                compare the current input against; e.g. UD [optional]
        
        {lexical,syntax,embedding,semantic,...} (bool): compute submodule features? [False]
    &#34;&#34;&#34;

    # lock = multiprocessing.Manager().Lock()

    # create output folder
    utils.io.log(&#39;creating output folder&#39;)
    # (sent_output_path,
    #  glove_words_output_path,
    #  glove_sents_output_path)
    output_dir = utils.io.create_output_paths(input_file,
                                              output_dir=output_dir,
                                              stop_words_file=stop_words_file)
    # with (output_dir / &#39;config.txt&#39;).open(&#39;w+&#39;) as f:
    #     print(args, file=f)

    utils.io.log(&#39;reading input sentences&#39;)
    UIDs, token_lists, sentences = utils.io.read_sentences(input_file, stop_words_file=stop_words_file)
    utils.io.log(&#39;---done--- reading input sentences&#39;)

    # surprisal_database = &#39;pickle/surprisal-3_dict.pkl&#39; # default: 3-gram surprisal
    # features_ignore_case = True
    # features_transform = [&#39;default&#39;, None, None] # format: [method, cols to log transform, cols to z-score] (if method is specified, the other two are ignored)

    # Get morpheme from polyglot library instead of library
    # TODO: where was this supposed to be used?
    # poly_morphemes = utils.get_poly_morpheme(flat_sentence_num, flat_token_list)

    if lexical:
        utils.io.log(&#39;*** running lexical submodule pipeline&#39;)
        _ = lexical.utils.load_databases(features=&#39;all&#39;)

        # lexical_features = [sentspace.lexical.get_features(sentence, identifier=UIDs[i])
        #                     for i, sentence in enumerate(tqdm(sentences, desc=&#39;Lexical pipeline&#39;))]
        lexical_features = utils.parallelize(lexical.get_features, sentences, UIDs,
                                             wrap_tqdm=True, desc=&#39;Lexical pipeline&#39;)

        lexical_out = output_dir / &#39;lexical&#39;
        lexical_out.mkdir(parents=True, exist_ok=True)

        # with (lexical_out/&#39;token-features.json&#39;).open(&#39;w&#39;) as f:
        #       json.dump(lexical_features, f)

        # lexical is a special case since it returns dicts per token (rather than per sentence)
        # so we want to flatten it so that pandas creates a sensible dataframe from it.
        token_df = pd.DataFrame(chain.from_iterable(lexical_features))

        utils.io.log(f&#39;outputting lexical token dataframe to {lexical_out}&#39;)
        if output_format == &#39;tsv&#39;:
            token_df.to_csv(lexical_out / &#39;token-features.tsv&#39;, sep=&#39;\t&#39;, index=False)
        if output_format == &#39;pkl&#39;:
            token_df.to_pickle(lexical_out / &#39;token-features.pkl.gz&#39;, protocol=5)

        utils.io.log(f&#39;--- finished lexical pipeline&#39;)

    if syntax:
        utils.io.log(&#39;*** running syntax submodule pipeline&#39;)
        syntax_features = [syntax.get_features(sentence, dlt=True, left_corner=True, identifier=UIDs[i])
                           for i, sentence in enumerate(tqdm(sentences, desc=&#39;Syntax pipeline&#39;))]
        # syntax_features = utils.parallelize(sentspace.syntax.get_features, sentences, UIDs,
        #                                     dlt=True, left_corner=True,
        #                                     wrap_tqdm=True, desc=&#39;Syntax pipeline&#39;)

        syntax_out = output_dir / &#39;syntax&#39;
        syntax_out.mkdir(parents=True, exist_ok=True)

        # with (syntax_out/&#39;features.json&#39;).open(&#39;w&#39;) as f:
        #       f.write(str(syntax_features))

        # put all features in the sentence df except the token-level ones
        token_syntax_features = {&#39;dlt&#39;, &#39;leftcorner&#39;}
        sentence_df = pd.DataFrame([{k: v for k, v in feature_dict.items() if k not in token_syntax_features}
                                    for feature_dict in syntax_features], index=UIDs)

        # output gives us dataframes corresponding to each token-level feature. we need to combine these
        # into a single dataframe
        # we use functools.reduce to apply the pd.concat function to all the dataframes and join dataframes
        # that contain different features for the same tokens
        # we use df.T.drop_duplicates().T to remove duplicate columns (&#39;token&#39;, &#39;sentence&#39;, &#39;index&#39; etc) that appear in
        # all/multiple dataframes as part of the standard output schema
        token_dfs = [reduce(lambda x, y: pd.concat([x, y], axis=1, sort=False),
                            (v for k, v in feature_dict.items() if k in token_syntax_features)).T.drop_duplicates().T
                     for feature_dict in syntax_features]
        token_df = reduce(lambda x, y: pd.concat([x, y], ignore_index=True), token_dfs)

        utils.io.log(f&#39;outputting syntax dataframes to {syntax_out}&#39;)
        if output_format == &#39;tsv&#39;:
            sentence_df.to_csv(syntax_out / &#39;sentence-features.tsv&#39;, sep=&#39;\t&#39;, index=False)
            token_df.to_csv(syntax_out / &#39;token-features.tsv&#39;, sep=&#39;\t&#39;, index=False)
        elif output_format == &#39;pkl&#39;:
            sentence_df.to_pickle(syntax_out / &#39;sentence-features.pkl.gz&#39;, protocol=5)
            token_df.to_pickle(syntax_out / &#39;token-features.pkl.gz&#39;, protocol=5)

        utils.io.log(f&#39;--- finished syntax pipeline&#39;)

    # Calculate PMI
    # utils.GrabNGrams(sent_rows,pmi_paths)
    # utils.pPMI(sent_rows, pmi_paths)
    # pdb.set_trace()

    if embedding:
        utils.io.log(&#39;*** running embedding submodule pipeline&#39;)
        # Get GloVE

        stripped_words = utils.text.strip_words(chain(*token_lists), method=&#39;punctuation&#39;)
        vocab = embedding.utils.get_vocab(stripped_words)
        _ = embedding.utils.load_embeddings(emb_file=&#39;glove.840B.300d.txt&#39;,
                                                      vocab=(*sorted(vocab),),
                                                      data_dir=emb_data_dir)

        # embedding_features = [sentspace.embedding.get_features(sentence, vocab=vocab, data_dir=emb_data_dir,
        #                                                        identifier=UIDs[i])
        #                        for i, sentence in enumerate(tqdm(sentences, desc=&#39;Embedding pipeline&#39;))]
        embedding_features = utils.parallelize(embedding.get_features, sentences, UIDs,
                                               vocab=vocab, data_dir=emb_data_dir,
                                               wrap_tqdm=True, desc=&#39;Embedding pipeline&#39;)

        embedding_out = output_dir / &#39;embedding&#39;
        embedding_out.mkdir(parents=True, exist_ok=True)

        sentence_df = pd.DataFrame([{k: v for k, v in feature_dict.items() if k != &#39;token_embeds&#39;}
                                    for feature_dict in embedding_features])

        utils.io.log(f&#39;outputting embedding dataframe(s) to {embedding_out}&#39;)
        if output_format == &#39;tsv&#39;:
            sentence_df.to_csv(embedding_out / &#39;sentence-features.tsv&#39;, sep=&#39;\t&#39;, index=False)
        elif output_format == &#39;pkl&#39;:
            sentence_df.to_pickle(embedding_out / &#39;sentence-features.pkl.gz&#39;, protocol=5)

        # token_dfs = [feature_dict[&#39;token_embeds&#39;] for feature_dict in embedding_features]
        # token_df = reduce(lambda x, y: pd.concat([x, y], ignore_index=True), token_dfs)

        # utils.io.log(f&#39;outputting embedding token dataframe to {embedding_out}&#39;)
        # token_df.to_csv(embedding_out / &#39;token-features.tsv&#39;, sep=&#39;\t&#39;, index=False)

        utils.io.log(f&#39;--- finished embedding pipeline&#39;)

    # Plot input data to benchmark data
    #utils.plot_usr_input_against_benchmark_dist_plots(df_benchmark, sent_embed)

    return output_dir</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#sentspace-001-c-2020-2021-evlab-mit-bcs-all-rights-reserved">Sentspace 0.0.1 (C) 2020-2021 EvLab , MIT BCS. All rights reserved.</a></li>
<li><a href="#sentspace">sentspace</a><ul>
<li><a href="#about-the-project">About The Project</a></li>
<li><a href="#documentation">Documentation</a></li>
<li><a href="#usage">Usage</a><ul>
<li><a href="#first-some-important-housekeeping-stuff">first, some important housekeeping stuff</a></li>
<li><a href="#next-running-the-container-automatically-built-and-deployed-to-docker-hub">next, running the container (automatically built and deployed to Docker hub)</a></li>
<li><a href="#submodules">Submodules</a><ul>
<li><a href="#syntax">syntax</a></li>
<li><a href="#lexical">lexical</a></li>
<li><a href="#embedding">embedding</a></li>
<li><a href="#semantic">semantic</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#license">License</a></li>
<li><a href="#contact">Contact</a></li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="sentspace.embedding" href="embedding/index.html">sentspace.embedding</a></code></li>
<li><code><a title="sentspace.lexical" href="lexical/index.html">sentspace.lexical</a></code></li>
<li><code><a title="sentspace.syntax" href="syntax/index.html">sentspace.syntax</a></code></li>
<li><code><a title="sentspace.utils" href="utils/index.html">sentspace.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sentspace.run_sentence_features_pipeline" href="#sentspace.run_sentence_features_pipeline">run_sentence_features_pipeline</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>