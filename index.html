<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>sentspace API documentation</title>
<meta name="description" content="Sentspace 0.0.1 (C) 2020-2021 EvLab &lt;evlab.mit.edu&gt;, MIT BCS. All rights reserved …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>sentspace</code></h1>
</header>
<section id="section-intro">
<h3 id="sentspace-001-c-2020-2021-evlab-mit-bcs-all-rights-reserved">Sentspace 0.0.1 (C) 2020-2021 EvLab <evlab.mit.edu>, MIT BCS. All rights reserved.</h3>
<p>Homepage: <a href="https://github.com/aalok-sathe/sentspace">https://github.com/aalok-sathe/sentspace</a></p>
<p>Authors (reverse alphabet.; insert <code>@</code> symbol for valid email):</p>
<ul>
<li>Greta Tuckute <code>&lt;gretatu % mit.edu&gt;</code></li>
<li>Aalok Sathe <code>&lt;asathe % mit.edu&gt;</code></li>
<li>Alvince Pongos <code>&lt;apongos % mit.edu&gt;</code></li>
<li>Josef Affourtit <code>&lt;jaffourt % mit.edu&gt;</code></li>
</ul>
<p>Please contact any of the following with questions about the code or license.</p>
<ul>
<li>Aalok Sathe <code>&lt;asathe % mit.edu&gt;</code> </li>
<li>Greta Tuckute <code>&lt;gretatu % mit.edu&gt;</code></li>
</ul>
<h1 id="sentspace">sentspace</h1>
<!-- ABOUT THE PROJECT -->
<h2 id="about-the-project">About The Project</h2>
<p><code><a title="sentspace" href="#sentspace">sentspace</a></code>
gives users a better understanding of the distribution of linguistic stimuli,
specifically, sentences, in comparison with large corpora. <code><a title="sentspace" href="#sentspace">sentspace</a></code> achieves
this using a collection of psycholinguistic datasets and linguistic features.
Imagine you have collected a set of sentences for use in a language experiment, or generated
sentences using an artificial neural network language model. How does your set of sentences compare to
naturally occurring sentences? What are the dimensions along which your sentences deviate from
<em>normal</em>?
<code><a title="sentspace" href="#sentspace">sentspace</a></code> provides you with numerical estimates of these values, as well as
allows you to visualize the high-dimensional space in a web-based application.</p>
<h2 id="documentation">Documentation</h2>
<p><a href="https://circleci.com/gh/aalok-sathe/sentspace/tree/main"><img alt="CircleCI" src="https://circleci.com/gh/aalok-sathe/sentspace/tree/main.svg?style=svg"></a></p>
<p>For more information, <a href="https://aalok-sathe.github.io/sentspace/index.html">visit the docs!</a></p>
<!-- request read access to the [project doc](https://docs.google.com/document/d/1O1M7T5Ji6KKRvDfI7KQXe_LJ7l9O6_OZA7TEaVP4f8E/edit#). -->
<h2 id="usage">Usage</h2>
<h3 id="cli">CLI</h3>
<p>Example: get lexical and embedding features for stimuli from a csv containing columns for 'sentence' and 'index'.</p>
<pre><code class="language-bash">python3 -m sentspace -lex 1 -syn 0 -emb 1 in/wsj_stimuli.csv
</code></pre>
<p>Example: get embedding features in a custom script</p>
<pre><code class="language-python">import sentspace

s = sentspace.Sentence.Sentence('The person purchased two mugs at the price of one.')
emb_feat = sentspace.embedding.get_features(s)
</code></pre>
<p>Example: parallelize getting features for multiple sentences using multithreading</p>
<pre><code class="language-python">import sentspace

sentences = [
    'Hello, how may I help you today?',
    'The person purchased three mugs at the price of five!',
    &quot;She's leaving home today.&quot;,
    'This is an example sentence we want features of.'
             ]

sentences = [*map(sentspace.Sentence.Sentence, sentences)]
lex_feat = sentspace.utils.parallelize(sentspace.lexical.get_features, sentences,
                                       wrap_tqdm=True, desc='Lexical features pipeline')
</code></pre>
<h2 id="installing">Installing</h2>
<h3 id="container-based-usage">Container-based usage</h3>
<p>To use the image as a container using <code>singularity/docker</code>:</p>
<h4 id="first-some-important-housekeeping-stuff"><strong>first, some important housekeeping stuff</strong></h4>
<ul>
<li>make sure you have <code>singularity</code>/<code>docker</code>, or load/install it otherwise</li>
<li><code>which singularity</code>
or
<code>which docker</code> </li>
<li>make sure you have set the ennvironment variables that specify where <code>singularity/docker</code> will cache its images. if you don't do this, <code>singularity</code> will make assumptions and you may end up with a full disk and an unresponsive server, if running on a server with filesystem restrictions. you should have about 5GB free space at the target location.</li>
</ul>
<h4 id="next-running-the-container-automatically-built-and-deployed-to-docker-hub"><strong>next, running the container</strong> (automatically built and deployed to Docker hub)</h4>
<p><a href="https://circleci.com/gh/aalok-sathe/sentspace/tree/circle-ci"><img alt="CircleCI" src="https://circleci.com/gh/aalok-sathe/sentspace/tree/circle-ci.svg?style=svg"></a></p>
<ul>
<li><code>singularity shell docker://aloxatel/sentspace:latest</code> (or alternatively, from the root of the repo, <code>bash singularity-shell.sh</code>). this step can take a while when you run it for the first time as it needs to download the image from docker hub and convert it to singularity image format (<code>.sif</code>). however, each subsequent run will execute rapidly. alternatively, use <a href="https://docs.docker.com/engine/reference/commandline/exec/">corresponding commands for Docker</a>.</li>
<li>now you are inside the container and ready to run Sentspace!</li>
</ul>
<p>For a complete list of options and default values, see the <code>help</code> page like so:</p>
<pre><code class="language-bash">python3 -m sentspace -h
</code></pre>
<h3 id="manual-dependency-install-not-officially-supported-needs-elevated-privileges">Manual dependency install (not officially supported; needs elevated privileges)</h3>
<pre><code class="language-bash"># optional (but recommended): 
# create a virtual environment using your favorite method (venv, conda, ...) 
# before any of the following

# install basic packages using apt (you likely already have these)
sudo apt update
sudo apt install python3.8 python3.8-dev python3-pip
sudo apt install python2.7 python2.7-dev 
sudo apt install build-essential git

# install ICU
DEBIAN_FRONTEND=&quot;noninteractive&quot; TZ=&quot;America/New_York&quot; sudo apt install python3-icu

# install ZS package separately (pypi install fails)
python3.8 -m pip install -U pip cython
git clone https://github.com/njsmith/zs
cd zs &amp;&amp; git checkout v0.10.0 &amp;&amp; pip install .

# install rest of the requirements using pip
cd .. # make sure you're in the sentspace/ directory
pip install -r ./requirements.txt
polyglot download morph2.en
</code></pre>
<h2 id="submodules">Submodules</h2>
<p>In general, each submodule implements a major class of features. You can run each module on its own by specifying its flag with the module call:</p>
<pre><code class="language-bash">python -m sentspace -lex 1 &lt;input_file_path&gt;
</code></pre>
<h4 id="sentspacelexical"><code><a title="sentspace.lexical" href="lexical/index.html">sentspace.lexical</a></code></h4>
<p>Obtain lexical (word-level) features that are not dependendent on the sentence context.
These features are returned on a word-by-word level and also averaged at the sentence level to provide each sentence a corresponding value.
- typical age of acquisition
- n-gram surprisal <code>n={1,2,3,4}</code>
- etc. (comprehensive list will be updated)</p>
<h4 id="sentspacesyntax"><code><a title="sentspace.syntax" href="syntax/index.html">sentspace.syntax</a></code></h4>
<p>Description pending</p>
<h4 id="sentspaceembedding"><code><a title="sentspace.embedding" href="embedding/index.html">sentspace.embedding</a></code></h4>
<p>Obtain high dimensional representations of sentences using word-embedding and contextualized encoder models.
- glove
- Huggingface model hub (<code>gpt2-xl</code>, <code>bert-base-uncased</code>)</p>
<h4 id="semantic"><code>semantic</code></h4>
<p>Multi-word features computed using partial or full sentence context.
- PMI (pointwise mutual information)
- Language model-based perplexity/surprisal
<em>Not Implemented yet</em></p>
<h2 id="contributing">Contributing</h2>
<p>Any contributions you make are <strong>greatly appreciated</strong>, and no contribution is <em>too small</em> to contribute.</p>
<ol>
<li>Fork the project using Github <a href="https://docs.github.com/en/get-started/quickstart/fork-a-repo">(how to fork)</a></li>
<li>Create your feature/patch branch (<code>git checkout -b feature/AmazingFeature</code>)</li>
<li>Make some changes! Implement your patch/feature. Test to make sure it works!</li>
<li>Commit your bhanges (<code>git commit -m 'Add some AmazingFeature'</code>)</li>
<li>Push the branch (<code>git push origin feature/AmazingFeature</code>)</li>
<li>Open a Pull Request (PR) and we will take a look!</li>
</ol>
<h2 id="license-contact">License &amp; Contact</h2>
<ul>
<li><code>gretatu % mit ^ edu</code></li>
<li><code>asathe % mit ^ edu</code></li>
</ul>
<p>(C) 2020-2021 EvLab, MIT BCS</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
    ### Sentspace 0.0.1 (C) 2020-2021 EvLab &lt;evlab.mit.edu&gt;, MIT BCS. All rights reserved.

    Homepage: https://github.com/aalok-sathe/sentspace

    Authors (reverse alphabet.; insert `@` symbol for valid email):
    
    - Greta Tuckute `&lt;gretatu % mit.edu&gt;`
    - Aalok Sathe `&lt;asathe % mit.edu&gt;`
    - Alvince Pongos `&lt;apongos % mit.edu&gt;`
    - Josef Affourtit `&lt;jaffourt % mit.edu&gt;`

    Please contact any of the following with questions about the code or license.
    
    - Aalok Sathe `&lt;asathe % mit.edu&gt;` 
    - Greta Tuckute `&lt;gretatu % mit.edu&gt;`

    .. include:: ../README.md
&#39;&#39;&#39;

# __pdoc__ = {&#39;semantic&#39;: False,
#             }


from collections import defaultdict
from pathlib import Path

import sentspace.utils as utils
import sentspace.syntax as syntax
import sentspace.lexical as lexical
import sentspace.embedding as embedding

from sentspace.Sentence import Sentence

import pandas as pd
from functools import reduce 
from itertools import chain
from tqdm import tqdm


def run_sentence_features_pipeline(input_file: str, stop_words_file: str = None,
                                   benchmark_file: str = None, output_dir: str = None,
                                   output_format: str = None, batch_size: int = 2_000,
                                   process_lexical: bool = False, process_syntax: bool = False,
                                   process_embedding: bool = False, process_semantic: bool = False,
                                   parallelize: bool = True,
                                   #
                                   emb_data_dir: str = None) -&gt; Path:
    &#34;&#34;&#34;
    Runs the full sentence features pipeline on the given input according to
    requested submodules (currently supported: `lexical`, `syntax`, `embedding`,
    indicated by boolean flags).
        
    Returns an instance of `Path` pointing to the output directory resulting from this
    run of the full pipeline. The output directory contains Pickled or TSVed pandas 
    DataFrames containing the requested features.


    Args:
        input_file (str): path to input text file containing sentences
                            one per line [required]
        stop_words_file (str): path to text file containing stopwords to filter
                                out, one per line [optional]
        benchmark_file (str): path to a file containing a benchmark corpus to
                                compare the current input against; e.g. UD [optional]
        
        {lexical,syntax,embedding,semantic,...} (bool): compute submodule features? [False]
    &#34;&#34;&#34;

    # lock = multiprocessing.Manager().Lock()

    # create output folder
    utils.io.log(&#39;creating output folder&#39;)
    output_dir = utils.io.create_output_paths(input_file,
                                              output_dir=output_dir,
                                              stop_words_file=stop_words_file)
    config_out = (output_dir / &#39;this_session_log.txt&#39;)
    # with config_out.open(&#39;a+&#39;) as f:
    #     print(args, file=f)

    utils.io.log(&#39;reading input sentences&#39;)
    sentences = utils.io.read_sentences(input_file, stop_words_file=stop_words_file)
    utils.io.log(&#39;---done--- reading input sentences&#39;)


    for part, sentence_batch in enumerate(tqdm(utils.io.get_batches(sentences, batch_size=batch_size), 
                                               desc=&#39;processing batches&#39;)):
        sentence_features_filestem = f&#39;sentence-features_part{part:0&gt;4}&#39;
        token_features_filestem = f&#39;token-features_part{part:0&gt;4}&#39;


        ################################################################################
        #### LEXICAL FEATURES ##########################################################
        ################################################################################
        if process_lexical:
            utils.io.log(&#39;*** running lexical submodule pipeline&#39;)
            _ = lexical.utils.load_databases(features=&#39;all&#39;)

            if parallelize:
                lexical_features = utils.parallelize(lexical.get_features, sentence_batch,
                                                    wrap_tqdm=True, desc=&#39;Lexical pipeline&#39;)
            else:
                lexical_features = [lexical.get_features(sentence)
                                    for _, sentence in enumerate(tqdm(sentence_batch, desc=&#39;Lexical pipeline&#39;))]

            lexical_out = output_dir / &#39;lexical&#39;
            lexical_out.mkdir(parents=True, exist_ok=True)
            utils.io.log(f&#39;outputting lexical token dataframe to {lexical_out}&#39;)

            # lexical is a special case since it returns dicts per token (rather than per sentence)
            # so we want to flatten it so that pandas creates a sensible dataframe from it.
            token_df = pd.DataFrame(chain.from_iterable(lexical_features))

            if output_format == &#39;tsv&#39;:
                token_df.to_csv(lexical_out / f&#39;{token_features_filestem}.tsv&#39;, sep=&#39;\t&#39;, index=True)
                token_df.groupby(&#39;sentence&#39;).mean().to_csv(lexical_out / f&#39;{sentence_features_filestem}.tsv&#39;, sep=&#39;\t&#39;, index=True)
            elif output_format == &#39;pkl&#39;:
                token_df.to_pickle(lexical_out / f&#39;{token_features_filestem}.pkl.gz&#39;, protocol=5)
                token_df.groupby(&#39;sentence&#39;).mean().to_pickle(lexical_out / f&#39;{sentence_features_filestem}.pkl.gz&#39;, protocol=5)
            else:
                raise ValueError(f&#39;output format {output_format} not known&#39;)

            utils.io.log(f&#39;--- finished lexical pipeline&#39;)


        ################################################################################
        #### SYNTAX FEATURES ###########################################################
        ################################################################################
        if process_syntax:
            utils.io.log(&#39;*** running syntax submodule pipeline&#39;)

            # as an exception, we do *not* parallelize syntax since the backend server is somehow unable to handle
            # multiple requests :(
            syntax_features = [syntax.get_features(sentence._raw, dlt=True, left_corner=True, identifier=sentence.uid)
                                                                        # !!! TODO:DEBUG
                            for i, sentence in enumerate(tqdm(sentences, desc=&#39;Syntax pipeline&#39;))]

            syntax_out = output_dir / &#39;syntax&#39;
            syntax_out.mkdir(parents=True, exist_ok=True)

            # put all features in the sentence df except the token-level ones
            token_syntax_features = {&#39;dlt&#39;, &#39;leftcorner&#39;}
            sentence_df = pd.DataFrame([{k: v for k, v in feature_dict.items() if k not in token_syntax_features}
                                        for feature_dict in syntax_features], index=[s.uid() for s in sentences])

            # output gives us dataframes corresponding to each token-level feature. we need to combine these
            # into a single dataframe
            # we use functools.reduce to apply the pd.concat function to all the dataframes and join dataframes
            # that contain different features for the same tokens
            token_dfs = [reduce(lambda x, y: pd.concat([x, y], axis=1, sort=False),
                                (v for k, v in feature_dict.items() if k in token_syntax_features))
                        for feature_dict in syntax_features]

            # by this point we have merged dataframes with tokens along a column (rather than just a sentence)
            # now we need to stack them on top of each other to have all tokens across all sentences in a single dataframe
            token_df = reduce(lambda x, y: pd.concat([x, y], ignore_index=True), token_dfs)
            token_df = token_df.loc[:, ~token_df.columns.duplicated()]

            utils.io.log(f&#39;outputting syntax dataframes to {syntax_out}&#39;)
            if output_format == &#39;tsv&#39;:
                sentence_df.to_csv(syntax_out / f&#39;{sentence_features_filestem}.tsv&#39;, sep=&#39;\t&#39;, index=True)
                token_df.to_csv(syntax_out / f&#39;{token_features_filestem}.tsv&#39;, sep=&#39;\t&#39;, index=False)
            elif output_format == &#39;pkl&#39;:
                sentence_df.to_pickle(syntax_out / f&#39;{sentence_features_filestem}.pkl.gz&#39;, protocol=5)
                token_df.to_pickle(syntax_out / f&#39;{token_features_filestem}.pkl.gz&#39;, protocol=5)

            utils.io.log(f&#39;--- finished syntax pipeline&#39;)

        # Calculate PMI
        # utils.GrabNGrams(sent_rows,pmi_paths)
        # utils.pPMI(sent_rows, pmi_paths)
        # pdb.set_trace()


        ################################################################################
        #### EMBEDDING FEATURES ########################################################
        ################################################################################            
        if process_embedding:
            utils.io.log(&#39;*** running embedding submodule pipeline&#39;)

            models_and_methods = [
                        # ({&#39;glove.840B.300d&#39;}, {&#39;mean&#39;, &#39;median&#39;}), 
                        # &#39;distilgpt2&#39;,
                        ({&#39;gpt2-xl&#39;}, {&#39;last&#39;}),
                        ({&#39;bert-base-uncased&#39;}, {&#39;first&#39;}),
                     ]
            
            vocab = None
            # does any of the
            if any(&#39;glove&#39; in model or &#39;word2vec&#39; in model for models, _ in models_and_methods for model in models):
                # get a vocabulary across all sentences given as input
                # as the first step, remove any punctuation from the tokens
                stripped_tokens = utils.text.strip_words(chain(*[s.tokens for s in sentences]), method=&#39;punctuation&#39;)
                # assemble a set of unique tokens
                vocab = set(stripped_tokens)
                # make a spurious function call so that loading glove is cached for subsequent calls
                # TODO allow specifying which glove/w2v version 
                _ = embedding.utils.load_embeddings(emb_file=&#39;glove.840B.300d.txt&#39;,
                                                    vocab=(*sorted(vocab),),
                                                    data_dir=emb_data_dir)

            if False and parallelize:
                embedding_features = utils.parallelize(embedding.get_features, 
                                                       sentences, models=models, 
                                                       vocab=vocab, data_dir=emb_data_dir,
                                                       wrap_tqdm=True, desc=&#39;Embedding pipeline&#39;)
            else:
                embedding_features = [embedding.get_features(sentence, models_and_methods=models_and_methods, 
                                                             vocab=vocab, data_dir=emb_data_dir)
                                      for i, sentence in enumerate(tqdm(sentences, desc=&#39;Embedding pipeline&#39;))]

            # a misc. stat being computed that needs to be handled better 
            # &#34;no&#34; means no. the stat below is counting how many sentences have NO content words (not to be confused with num. content words)
            no_content_words = len(sentences)-sum(any(s.content_words) for s in sentences)

            utils.io.log(f&#39;sentences without any content words: {no_content_words}/{len(sentences)}; {no_content_words/len(sentences):.2f}&#39;)

            embedding_out = output_dir / &#39;embedding&#39;
            embedding_out.mkdir(parents=True, exist_ok=True)
            
            
            # now we want to output stuff from embedding_features (which is returned by the embedding pipeline)
            # into nicely formatted dataframes.
            # the structure of what is returned by the embedding pipeline is like so:
            #   gpt2-xl:
            #       last: [...] flat multiindexed Pandas series with (layer, dim) as the two indices
            #       mean: 
            #   glove:
            #       mean: [...] flat multiindexed Pandas series with trivially a single layer and 300d, so (1, 300) as the two indices
            # etc.

            # a set of all the models in use
            all_models_methods = {model_name: feature_dict[&#39;features&#39;][model_name].keys() 
                                  for feature_dict in embedding_features 
                                    for model_name in feature_dict[&#39;features&#39;]}

            print(all_models_methods)

            # we want to output BY MODEL
            for model_name in all_models_methods:
                # and BY METHOD
                for method in all_models_methods[model_name]:
                    # each `feature_dict` corresponds to ONE sentence

                    collected = []
                    for feature_dict in embedding_features:

                        # all the keys that contain information such as the sentence, UID, filters used etc,
                        # except for the actual representations obtained from various models.
                        # we need to know this so we can package all this information together with the outputs by model and method
                        metadata_keys = {*feature_dict.keys()} - {&#39;features&#39;} # setminus operator
                        # make a copy of the feature_dict for this sentence excluding the representations themselves
                        meta_df = {key: feature_dict[key] for key in metadata_keys}
                        meta_df.update({&#39;model_name&#39;: model_name, &#39;aggregation&#39;: method})
                        meta_df = pd.DataFrame(meta_df, index=[feature_dict[&#39;index&#39;]])
                        meta_df.columns = pd.MultiIndex.from_product([[&#39;metadata&#39;], meta_df.columns, [&#39;&#39;]])

                        # model_name -&gt; method -&gt; reprs
                        pooled_reprs = feature_dict[&#39;features&#39;]
                        flattened_repr = pooled_reprs[model_name][method]

                        collected += [pd.concat([meta_df, flattened_repr], axis=1)]

                    # create further subdirectories by model and aggregation method
                    (embedding_out / model_name / method).mkdir(parents=True, exist_ok=True)

                    sentence_df = pd.concat(collected, axis=0)

                    utils.io.log(f&#39;outputting embedding dataframes for {model_name}-{method} to {embedding_out}&#39;)
                    if output_format == &#39;tsv&#39;:
                        sentence_df.to_csv(embedding_out / model_name / method / f&#39;{sentence_features_filestem}.tsv&#39;, sep=&#39;\t&#39;, index=True)
                        # token_df.to_csv(embedding_out / f&#39;{token_features_filestem}.tsv&#39;, sep=&#39;\t&#39;, index=False)
                    elif output_format == &#39;pkl&#39;:
                        sentence_df.to_pickle(embedding_out / model_name / method / f&#39;{sentence_features_filestem}.pkl.gz&#39;, protocol=5)
                        # token_df.to_pickle(embedding_out / f&#39;{token_features_filestem}.pkl.gz&#39;, protocol=5)


            utils.io.log(f&#39;--- finished embedding pipeline&#39;)

        # Plot input data to benchmark data
        #utils.plot_usr_input_against_benchmark_dist_plots(df_benchmark, sent_embed)

        if process_semantic:
            pass


    ################################################################################
    #### \end{run_sentence_features_pipeline} ######################################
    ################################################################################
    return output_dir</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="sentspace.Sentence" href="Sentence.html">sentspace.Sentence</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="sentspace.embedding" href="embedding/index.html">sentspace.embedding</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="sentspace.lexical" href="lexical/index.html">sentspace.lexical</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="sentspace.syntax" href="syntax/index.html">sentspace.syntax</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="sentspace.utils" href="utils/index.html">sentspace.utils</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="sentspace.run_sentence_features_pipeline"><code class="name flex">
<span>def <span class="ident">run_sentence_features_pipeline</span></span>(<span>input_file: str, stop_words_file: str = None, benchmark_file: str = None, output_dir: str = None, output_format: str = None, batch_size: int = 2000, process_lexical: bool = False, process_syntax: bool = False, process_embedding: bool = False, process_semantic: bool = False, parallelize: bool = True, emb_data_dir: str = None) ‑> pathlib.Path</span>
</code></dt>
<dd>
<div class="desc"><p>Runs the full sentence features pipeline on the given input according to
requested submodules (currently supported: <code><a title="sentspace.lexical" href="lexical/index.html">sentspace.lexical</a></code>, <code><a title="sentspace.syntax" href="syntax/index.html">sentspace.syntax</a></code>, <code><a title="sentspace.embedding" href="embedding/index.html">sentspace.embedding</a></code>,
indicated by boolean flags).</p>
<p>Returns an instance of <code>Path</code> pointing to the output directory resulting from this
run of the full pipeline. The output directory contains Pickled or TSVed pandas
DataFrames containing the requested features.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>input_file</code></strong> :&ensp;<code>str</code></dt>
<dd>path to input text file containing sentences
one per line [required]</dd>
<dt><strong><code>stop_words_file</code></strong> :&ensp;<code>str</code></dt>
<dd>path to text file containing stopwords to filter
out, one per line [optional]</dd>
<dt><strong><code>benchmark_file</code></strong> :&ensp;<code>str</code></dt>
<dd>path to a file containing a benchmark corpus to
compare the current input against; e.g. UD [optional]</dd>
</dl>
<p>{lexical,syntax,embedding,semantic,&hellip;} (bool): compute submodule features? [False]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_sentence_features_pipeline(input_file: str, stop_words_file: str = None,
                                   benchmark_file: str = None, output_dir: str = None,
                                   output_format: str = None, batch_size: int = 2_000,
                                   process_lexical: bool = False, process_syntax: bool = False,
                                   process_embedding: bool = False, process_semantic: bool = False,
                                   parallelize: bool = True,
                                   #
                                   emb_data_dir: str = None) -&gt; Path:
    &#34;&#34;&#34;
    Runs the full sentence features pipeline on the given input according to
    requested submodules (currently supported: `lexical`, `syntax`, `embedding`,
    indicated by boolean flags).
        
    Returns an instance of `Path` pointing to the output directory resulting from this
    run of the full pipeline. The output directory contains Pickled or TSVed pandas 
    DataFrames containing the requested features.


    Args:
        input_file (str): path to input text file containing sentences
                            one per line [required]
        stop_words_file (str): path to text file containing stopwords to filter
                                out, one per line [optional]
        benchmark_file (str): path to a file containing a benchmark corpus to
                                compare the current input against; e.g. UD [optional]
        
        {lexical,syntax,embedding,semantic,...} (bool): compute submodule features? [False]
    &#34;&#34;&#34;

    # lock = multiprocessing.Manager().Lock()

    # create output folder
    utils.io.log(&#39;creating output folder&#39;)
    output_dir = utils.io.create_output_paths(input_file,
                                              output_dir=output_dir,
                                              stop_words_file=stop_words_file)
    config_out = (output_dir / &#39;this_session_log.txt&#39;)
    # with config_out.open(&#39;a+&#39;) as f:
    #     print(args, file=f)

    utils.io.log(&#39;reading input sentences&#39;)
    sentences = utils.io.read_sentences(input_file, stop_words_file=stop_words_file)
    utils.io.log(&#39;---done--- reading input sentences&#39;)


    for part, sentence_batch in enumerate(tqdm(utils.io.get_batches(sentences, batch_size=batch_size), 
                                               desc=&#39;processing batches&#39;)):
        sentence_features_filestem = f&#39;sentence-features_part{part:0&gt;4}&#39;
        token_features_filestem = f&#39;token-features_part{part:0&gt;4}&#39;


        ################################################################################
        #### LEXICAL FEATURES ##########################################################
        ################################################################################
        if process_lexical:
            utils.io.log(&#39;*** running lexical submodule pipeline&#39;)
            _ = lexical.utils.load_databases(features=&#39;all&#39;)

            if parallelize:
                lexical_features = utils.parallelize(lexical.get_features, sentence_batch,
                                                    wrap_tqdm=True, desc=&#39;Lexical pipeline&#39;)
            else:
                lexical_features = [lexical.get_features(sentence)
                                    for _, sentence in enumerate(tqdm(sentence_batch, desc=&#39;Lexical pipeline&#39;))]

            lexical_out = output_dir / &#39;lexical&#39;
            lexical_out.mkdir(parents=True, exist_ok=True)
            utils.io.log(f&#39;outputting lexical token dataframe to {lexical_out}&#39;)

            # lexical is a special case since it returns dicts per token (rather than per sentence)
            # so we want to flatten it so that pandas creates a sensible dataframe from it.
            token_df = pd.DataFrame(chain.from_iterable(lexical_features))

            if output_format == &#39;tsv&#39;:
                token_df.to_csv(lexical_out / f&#39;{token_features_filestem}.tsv&#39;, sep=&#39;\t&#39;, index=True)
                token_df.groupby(&#39;sentence&#39;).mean().to_csv(lexical_out / f&#39;{sentence_features_filestem}.tsv&#39;, sep=&#39;\t&#39;, index=True)
            elif output_format == &#39;pkl&#39;:
                token_df.to_pickle(lexical_out / f&#39;{token_features_filestem}.pkl.gz&#39;, protocol=5)
                token_df.groupby(&#39;sentence&#39;).mean().to_pickle(lexical_out / f&#39;{sentence_features_filestem}.pkl.gz&#39;, protocol=5)
            else:
                raise ValueError(f&#39;output format {output_format} not known&#39;)

            utils.io.log(f&#39;--- finished lexical pipeline&#39;)


        ################################################################################
        #### SYNTAX FEATURES ###########################################################
        ################################################################################
        if process_syntax:
            utils.io.log(&#39;*** running syntax submodule pipeline&#39;)

            # as an exception, we do *not* parallelize syntax since the backend server is somehow unable to handle
            # multiple requests :(
            syntax_features = [syntax.get_features(sentence._raw, dlt=True, left_corner=True, identifier=sentence.uid)
                                                                        # !!! TODO:DEBUG
                            for i, sentence in enumerate(tqdm(sentences, desc=&#39;Syntax pipeline&#39;))]

            syntax_out = output_dir / &#39;syntax&#39;
            syntax_out.mkdir(parents=True, exist_ok=True)

            # put all features in the sentence df except the token-level ones
            token_syntax_features = {&#39;dlt&#39;, &#39;leftcorner&#39;}
            sentence_df = pd.DataFrame([{k: v for k, v in feature_dict.items() if k not in token_syntax_features}
                                        for feature_dict in syntax_features], index=[s.uid() for s in sentences])

            # output gives us dataframes corresponding to each token-level feature. we need to combine these
            # into a single dataframe
            # we use functools.reduce to apply the pd.concat function to all the dataframes and join dataframes
            # that contain different features for the same tokens
            token_dfs = [reduce(lambda x, y: pd.concat([x, y], axis=1, sort=False),
                                (v for k, v in feature_dict.items() if k in token_syntax_features))
                        for feature_dict in syntax_features]

            # by this point we have merged dataframes with tokens along a column (rather than just a sentence)
            # now we need to stack them on top of each other to have all tokens across all sentences in a single dataframe
            token_df = reduce(lambda x, y: pd.concat([x, y], ignore_index=True), token_dfs)
            token_df = token_df.loc[:, ~token_df.columns.duplicated()]

            utils.io.log(f&#39;outputting syntax dataframes to {syntax_out}&#39;)
            if output_format == &#39;tsv&#39;:
                sentence_df.to_csv(syntax_out / f&#39;{sentence_features_filestem}.tsv&#39;, sep=&#39;\t&#39;, index=True)
                token_df.to_csv(syntax_out / f&#39;{token_features_filestem}.tsv&#39;, sep=&#39;\t&#39;, index=False)
            elif output_format == &#39;pkl&#39;:
                sentence_df.to_pickle(syntax_out / f&#39;{sentence_features_filestem}.pkl.gz&#39;, protocol=5)
                token_df.to_pickle(syntax_out / f&#39;{token_features_filestem}.pkl.gz&#39;, protocol=5)

            utils.io.log(f&#39;--- finished syntax pipeline&#39;)

        # Calculate PMI
        # utils.GrabNGrams(sent_rows,pmi_paths)
        # utils.pPMI(sent_rows, pmi_paths)
        # pdb.set_trace()


        ################################################################################
        #### EMBEDDING FEATURES ########################################################
        ################################################################################            
        if process_embedding:
            utils.io.log(&#39;*** running embedding submodule pipeline&#39;)

            models_and_methods = [
                        # ({&#39;glove.840B.300d&#39;}, {&#39;mean&#39;, &#39;median&#39;}), 
                        # &#39;distilgpt2&#39;,
                        ({&#39;gpt2-xl&#39;}, {&#39;last&#39;}),
                        ({&#39;bert-base-uncased&#39;}, {&#39;first&#39;}),
                     ]
            
            vocab = None
            # does any of the
            if any(&#39;glove&#39; in model or &#39;word2vec&#39; in model for models, _ in models_and_methods for model in models):
                # get a vocabulary across all sentences given as input
                # as the first step, remove any punctuation from the tokens
                stripped_tokens = utils.text.strip_words(chain(*[s.tokens for s in sentences]), method=&#39;punctuation&#39;)
                # assemble a set of unique tokens
                vocab = set(stripped_tokens)
                # make a spurious function call so that loading glove is cached for subsequent calls
                # TODO allow specifying which glove/w2v version 
                _ = embedding.utils.load_embeddings(emb_file=&#39;glove.840B.300d.txt&#39;,
                                                    vocab=(*sorted(vocab),),
                                                    data_dir=emb_data_dir)

            if False and parallelize:
                embedding_features = utils.parallelize(embedding.get_features, 
                                                       sentences, models=models, 
                                                       vocab=vocab, data_dir=emb_data_dir,
                                                       wrap_tqdm=True, desc=&#39;Embedding pipeline&#39;)
            else:
                embedding_features = [embedding.get_features(sentence, models_and_methods=models_and_methods, 
                                                             vocab=vocab, data_dir=emb_data_dir)
                                      for i, sentence in enumerate(tqdm(sentences, desc=&#39;Embedding pipeline&#39;))]

            # a misc. stat being computed that needs to be handled better 
            # &#34;no&#34; means no. the stat below is counting how many sentences have NO content words (not to be confused with num. content words)
            no_content_words = len(sentences)-sum(any(s.content_words) for s in sentences)

            utils.io.log(f&#39;sentences without any content words: {no_content_words}/{len(sentences)}; {no_content_words/len(sentences):.2f}&#39;)

            embedding_out = output_dir / &#39;embedding&#39;
            embedding_out.mkdir(parents=True, exist_ok=True)
            
            
            # now we want to output stuff from embedding_features (which is returned by the embedding pipeline)
            # into nicely formatted dataframes.
            # the structure of what is returned by the embedding pipeline is like so:
            #   gpt2-xl:
            #       last: [...] flat multiindexed Pandas series with (layer, dim) as the two indices
            #       mean: 
            #   glove:
            #       mean: [...] flat multiindexed Pandas series with trivially a single layer and 300d, so (1, 300) as the two indices
            # etc.

            # a set of all the models in use
            all_models_methods = {model_name: feature_dict[&#39;features&#39;][model_name].keys() 
                                  for feature_dict in embedding_features 
                                    for model_name in feature_dict[&#39;features&#39;]}

            print(all_models_methods)

            # we want to output BY MODEL
            for model_name in all_models_methods:
                # and BY METHOD
                for method in all_models_methods[model_name]:
                    # each `feature_dict` corresponds to ONE sentence

                    collected = []
                    for feature_dict in embedding_features:

                        # all the keys that contain information such as the sentence, UID, filters used etc,
                        # except for the actual representations obtained from various models.
                        # we need to know this so we can package all this information together with the outputs by model and method
                        metadata_keys = {*feature_dict.keys()} - {&#39;features&#39;} # setminus operator
                        # make a copy of the feature_dict for this sentence excluding the representations themselves
                        meta_df = {key: feature_dict[key] for key in metadata_keys}
                        meta_df.update({&#39;model_name&#39;: model_name, &#39;aggregation&#39;: method})
                        meta_df = pd.DataFrame(meta_df, index=[feature_dict[&#39;index&#39;]])
                        meta_df.columns = pd.MultiIndex.from_product([[&#39;metadata&#39;], meta_df.columns, [&#39;&#39;]])

                        # model_name -&gt; method -&gt; reprs
                        pooled_reprs = feature_dict[&#39;features&#39;]
                        flattened_repr = pooled_reprs[model_name][method]

                        collected += [pd.concat([meta_df, flattened_repr], axis=1)]

                    # create further subdirectories by model and aggregation method
                    (embedding_out / model_name / method).mkdir(parents=True, exist_ok=True)

                    sentence_df = pd.concat(collected, axis=0)

                    utils.io.log(f&#39;outputting embedding dataframes for {model_name}-{method} to {embedding_out}&#39;)
                    if output_format == &#39;tsv&#39;:
                        sentence_df.to_csv(embedding_out / model_name / method / f&#39;{sentence_features_filestem}.tsv&#39;, sep=&#39;\t&#39;, index=True)
                        # token_df.to_csv(embedding_out / f&#39;{token_features_filestem}.tsv&#39;, sep=&#39;\t&#39;, index=False)
                    elif output_format == &#39;pkl&#39;:
                        sentence_df.to_pickle(embedding_out / model_name / method / f&#39;{sentence_features_filestem}.pkl.gz&#39;, protocol=5)
                        # token_df.to_pickle(embedding_out / f&#39;{token_features_filestem}.pkl.gz&#39;, protocol=5)


            utils.io.log(f&#39;--- finished embedding pipeline&#39;)

        # Plot input data to benchmark data
        #utils.plot_usr_input_against_benchmark_dist_plots(df_benchmark, sent_embed)

        if process_semantic:
            pass


    ################################################################################
    #### \end{run_sentence_features_pipeline} ######################################
    ################################################################################
    return output_dir</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#sentspace-001-c-2020-2021-evlab-mit-bcs-all-rights-reserved">Sentspace 0.0.1 (C) 2020-2021 EvLab , MIT BCS. All rights reserved.</a></li>
<li><a href="#sentspace">sentspace</a><ul>
<li><a href="#about-the-project">About The Project</a></li>
<li><a href="#documentation">Documentation</a></li>
<li><a href="#usage">Usage</a><ul>
<li><a href="#cli">CLI</a></li>
</ul>
</li>
<li><a href="#installing">Installing</a><ul>
<li><a href="#container-based-usage">Container-based usage</a><ul>
<li><a href="#first-some-important-housekeeping-stuff">first, some important housekeeping stuff</a></li>
<li><a href="#next-running-the-container-automatically-built-and-deployed-to-docker-hub">next, running the container (automatically built and deployed to Docker hub)</a></li>
</ul>
</li>
<li><a href="#manual-dependency-install-not-officially-supported-needs-elevated-privileges">Manual dependency install (not officially supported; needs elevated privileges)</a></li>
</ul>
</li>
<li><a href="#submodules">Submodules</a><ul>
<li><a href="#lexical">lexical</a></li>
<li><a href="#syntax">syntax</a></li>
<li><a href="#embedding">embedding</a></li>
<li><a href="#semantic">semantic</a></li>
</ul>
</li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#license-contact">License &amp; Contact</a></li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="sentspace.Sentence" href="Sentence.html">sentspace.Sentence</a></code></li>
<li><code><a title="sentspace.embedding" href="embedding/index.html">sentspace.embedding</a></code></li>
<li><code><a title="sentspace.lexical" href="lexical/index.html">sentspace.lexical</a></code></li>
<li><code><a title="sentspace.syntax" href="syntax/index.html">sentspace.syntax</a></code></li>
<li><code><a title="sentspace.utils" href="utils/index.html">sentspace.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="sentspace.run_sentence_features_pipeline" href="#sentspace.run_sentence_features_pipeline">run_sentence_features_pipeline</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>